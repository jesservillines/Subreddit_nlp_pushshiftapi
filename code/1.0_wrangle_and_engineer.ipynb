{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access and combine datafiles saved in ./data folder into a master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockMarket    20000\n",
      "stocks         20000\n",
      "Name: subreddit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "filenames = ['stockmarket', 'stocks']\n",
    "\n",
    "masterframe = []\n",
    "for filename in filenames:\n",
    "    frame = []\n",
    "    masterframe.append(frame)\n",
    "    for filenumber in range(1,21):\n",
    "        filepath = f'../data/{filename}_{filenumber}.csv'\n",
    "        file = pd.read_csv(filepath)\n",
    "        dataframe = file[['subreddit', 'title', 'selftext', 'score', 'created_utc']]\n",
    "        frame.append(dataframe)\n",
    "            \n",
    "assert len(masterframe) == len(filenames)\n",
    "\n",
    "reddit = []\n",
    "for i in masterframe:\n",
    "    data = pd.concat(i)\n",
    "    reddit.append(data)\n",
    "reddit_df = pd.concat(reddit)\n",
    "print(reddit_df['subreddit'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   subreddit    40000 non-null  object\n",
      " 1   title        40000 non-null  object\n",
      " 2   selftext     38495 non-null  object\n",
      " 3   score        40000 non-null  int64 \n",
      " 4   created_utc  40000 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "reddit_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values = subreddit         0\n",
      "title             0\n",
      "selftext       1505\n",
      "score             0\n",
      "created_utc       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Null values = {reddit_df.isna().sum()}')\n",
    "reddit_df = reddit_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stocks         19538\n",
       "StockMarket    18957\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datetime column by converting utc to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['datetime'] = pd.to_datetime(reddit_df['created_utc'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2021-01-02 00:38:37\n",
       "Name: datetime, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['datetime'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999   2019-01-29 19:42:16\n",
       "Name: datetime, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['datetime'].tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out [removed] and [deleted] posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[removed]    11885\n",
       "[deleted]      162\n",
       "Name: selftext, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['selftext'].value_counts()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[(reddit_df['selftext']!= '[removed]') & (reddit_df['selftext']!= '[deleted]') ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26448, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stocks         15984\n",
       "StockMarket    10464\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a label column by setting subreddit stocks to 0 and subreddit stockmarket to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label column\n",
    "reddit_df['subreddit'] = reddit_df['subreddit'].map({\"StockMarket\":1 , 'stocks':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15984\n",
       "1    10464\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select reddits with score of greater than 0. These will be more representative of good posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[reddit_df['score'] >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['all_text'] = reddit_df['selftext'] + reddit_df['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply regex on all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['all_text'] = reddit_df['all_text'].map(lambda x: re.sub(r'\\W+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the oldest Stocks records to make the sets equal. There are many more posts on the sotck_market page. 2019 is the 2nd best feature for Stocks. Reducing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = reddit_df[reddit_df['subreddit'] ==0]\n",
    "stock_market = reddit_df[reddit_df['subreddit'] ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13709\n",
      "10018\n"
     ]
    }
   ],
   "source": [
    "print(len(stocks))\n",
    "print(len(stock_market))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "stocks = reddit_df[reddit_df['subreddit'] ==0]\n",
    "stock_market = reddit_df[reddit_df['subreddit'] ==1]\n",
    "stocks.reset_index(inplace=True)\n",
    "stock_market.reset_index(inplace=True)\n",
    "stocks.drop(stocks.tail(3691).index,inplace=True)\n",
    "assert len(stocks) == len(stock_market)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2020-02-17 16:44:21\n",
      "Name: datetime, dtype: datetime64[ns]\n",
      "10017   2019-06-05 16:51:20\n",
      "Name: datetime, dtype: datetime64[ns]\n",
      "0   2021-01-02 00:14:04\n",
      "Name: datetime, dtype: datetime64[ns]\n",
      "10017   2020-01-15 05:09:44\n",
      "Name: datetime, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(stocks['datetime'].head(1))\n",
    "print(stocks['datetime'].tail(1))\n",
    "print(stock_market['datetime'].head(1))\n",
    "print(stock_market['datetime'].tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = [stocks, stock_market]\n",
    "model_df = pd.concat(all_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Title and Selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['all_text'] = model_df['title'] + model_df['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words added to sentiment vocabulary from expert knowledge and logistic regression coefficient importance. These words were added to customize our sentiment analysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "new_words = {\n",
    "    'buy': 4.0,\n",
    "    'vaccine': 4.0,\n",
    "    'stimulus': 1.0,\n",
    "    'bull': 4.0,\n",
    "    'bullish': 4.0,\n",
    "    'bear': -4.0,\n",
    "    'bearish': -4.0,\n",
    "    'sell': -4.0,\n",
    "    'unstable': -4.0,\n",
    "    'covid': -4.0,\n",
    "    'pandemic': -4.0,\n",
    "    'Wuhan':-10,\n",
    "    'ignorant': -4,\n",
    "    'lockdown': -4,\n",
    "    'covid19': -4\n",
    "}\n",
    "\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "\n",
    "SIA.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the sentiment analyzer\n",
    "\n",
    "# Write a function to get the compound sentiment scores for a post\n",
    "def get_compound_sentiment(post):\n",
    "    return SIA.polarity_scores(post)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['sentiment'] = model_df['all_text'].apply(get_compound_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove some popular words that add no meaning or won't be applicable in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = ['https', 'com', '2019', 'amp']\n",
    "\n",
    "for word in stop_word:\n",
    "    model_df['all_text'] = model_df['all_text'].str.replace(word, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['all_text'] = model_df['all_text'].str.replace('stocks', 'stock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write model file to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>datetime</th>\n",
       "      <th>all_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Taxes if I don't have any income?</td>\n",
       "      <td>So I'm a college student and I'm fooling aroun...</td>\n",
       "      <td>1</td>\n",
       "      <td>1581957861</td>\n",
       "      <td>2020-02-17 16:44:21</td>\n",
       "      <td>Taxes if I don't have any ine?So I'm a college...</td>\n",
       "      <td>-0.1680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Those of you that keep fairly long watchlists,...</td>\n",
       "      <td>Now that I'm learning to use screeners and che...</td>\n",
       "      <td>1</td>\n",
       "      <td>1581957764</td>\n",
       "      <td>2020-02-17 16:42:44</td>\n",
       "      <td>Those of you that keep fairly long watchlists,...</td>\n",
       "      <td>0.8816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Covered Calls (Vanguard)</td>\n",
       "      <td>I'm looking for some feedback from people that...</td>\n",
       "      <td>1</td>\n",
       "      <td>1581956404</td>\n",
       "      <td>2020-02-17 16:20:04</td>\n",
       "      <td>Covered Calls (Vanguard)I'm looking for some f...</td>\n",
       "      <td>0.7650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  subreddit                                              title  \\\n",
       "0      0          0                  Taxes if I don't have any income?   \n",
       "1      1          0  Those of you that keep fairly long watchlists,...   \n",
       "2      2          0                           Covered Calls (Vanguard)   \n",
       "\n",
       "                                            selftext  score  created_utc  \\\n",
       "0  So I'm a college student and I'm fooling aroun...      1   1581957861   \n",
       "1  Now that I'm learning to use screeners and che...      1   1581957764   \n",
       "2  I'm looking for some feedback from people that...      1   1581956404   \n",
       "\n",
       "             datetime                                           all_text  \\\n",
       "0 2020-02-17 16:44:21  Taxes if I don't have any ine?So I'm a college...   \n",
       "1 2020-02-17 16:42:44  Those of you that keep fairly long watchlists,...   \n",
       "2 2020-02-17 16:20:04  Covered Calls (Vanguard)I'm looking for some f...   \n",
       "\n",
       "   sentiment  \n",
       "0    -0.1680  \n",
       "1     0.8816  \n",
       "2     0.7650  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('../data/model_df.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
